---
layout: post
title: "섹션6. Softmax Regression"
date: 2019-02-12 18:40:00
image: '/assets/img/ML/intro.jpeg'
description: ML study
category: 'Machine Learning'
tags:
- Machine Learning
twitter_text: Lorem ipsum dolor sit amet, consectetur adipisicing elit.
introduction: A summary of what i studied
---


## 섹션6. Logistic(Regression) Classification

### 00xSoftmax Classification(Multinomial)

이전의 Logistic Regression이 Binary Classificatino이였다면, Softmax Classification은 Multinomial Classification이다. 

### 01xMultinomial Classification

![problem](/assets/img/ML/section6/fig1.PNG "Multinomial Classification")
<center><font size="0.5em">(Fig 1. Multinomial Classification)</font></center><br>

Multinomail은 Logistic Regression의 메커니즘을 그대로 가져온다. 대신 여러개의 Hypothesis를 이용하여 클래스 간을 구분짓는다.

### 02xMultinomial Classification Mechanism

![problem](/assets/img/ML/section6/fig2.PNG "Multinomial Classification")
<center><font size="0.5em">(Fig 2. Multinomial Classification)</font></center><br>

![problem](/assets/img/ML/section6/fig3.PNG "Multinomial Classification")
<center><font size="0.5em">(Fig 3. Multinomial Classification Hypothesis)</font></center><br>
위 처럼 여러개의 가설은 각각 사용하는 것이 아닌 2차 배열 속 하나의 벡터로 나타냄으로써 행렬의 곱으로 계산한다.

### 03xSoftmax Algorithm's Cost Function

![problem](/assets/img/ML/section6/fig4.PNG "Multinomial Classification Cost Function/Cross-entropy")
<center><font size="0.5em">(Fig 4. Multinomial Classification Cost Function/Cross-entropy)</font></center><br>

log값은 0에 가까울 수록 음의 무한대로 수렴한다. 따라사 Li가 1인 i에 해당하는 Yi값이 0에 가까울 수록 Cost Function은 커진다.



