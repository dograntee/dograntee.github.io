---
layout: post
title: "섹션5. Logistic(Regression) Classification"
date: 2019-02-11 02:08:00
image: '/assets/img/ML/intro.jpeg'
description: ML study
category: 'Machine Learning'
tags:
- Machine Learning
twitter_text: Lorem ipsum dolor sit amet, consectetur adipisicing elit.
introduction: A summary of what i studied
---


## 섹션5. Logistic(Regression) Classification

### 00xClassification

![problem](/assets/img/ML/section5/fig1.PNG "Radiology")
<center><font size="0.5em">(Fig 1. Radiology & Finance)</font></center><br>

 이전에 언급한 Regression이 수치를 예측하는 것이라면, Classification은 카테고리를 예측하는 모델이다. 예를 들면 스팸 메일 감지, 페이스북 피드, 신용카드 거짓거래 탐지, 주식 거래(매매 여부 판정),  등이 있다.



### 01xClassification using Encoding(0 or 1)
 - Spam Detection : Spam(1) or Ham(0)
 - Facebook feed : show(1) or Hide(0)
 - Credit Card Fraudulent Transaction detection : Legitimate(0) or Fraud(1)

### 02xLinear Regression으로 Classification을 하면 생기는 문제점

![problem](/assets/img/ML/section5/fig2.PNG "Linear")
<center><font size="0.5em">(Fig 2. Linear?)</font></center><br>

 - 공부 시간에 따른 Pass(1) or Fail(0) 데이터를 가지고 학습할 때, Linear Regression에서 예측한 값이 0.5를 넘는지 안넘는지에 따라 Pass or Fail을 예측한다면 가설식은 Linear하기 때문에 학습 데이터 중 오랜시간 공부하여 Pass(1)을 얻은 데이터가 있을 때 가설식의 기울기가 낮아져 실제 Pass를 받을만한 시간동안 공부하여도 Fail로 예측할 수 있다. 

### 03xLogistic Hypothesis(sigmoid)

![problem](/assets/img/ML/section5/fig3.png "Sigmoid Function")
<center><font size="0.5em">(Fig 3. Sigmoid Function)</font></center><br>

Sigmoid함수는 X값이 커질수록 1에 수렴하고 X값이 작아질수록 0에 수렴하는 꼴을 가진다. 이때 X값을 기존 Linear Hypothesis식을 대입하여 이를 Hypothesis로 사용함으로써, 기존 Linear함수가 Classification을 위해 사용될 때 0~1 사이의 범위를 초과하거나 한쪽으로 크게 기준이 편향되는 것을 완화할 수 있다. 

### 04xLogistic Regression의 Cost함수

![problem](/assets/img/ML/section5/fig4.PNG "Cost function")
<center><font size="0.5em">(Fig 4. Cost function)</font></center><br>

Linear regression처럼 Logistic regression의 Cost함수를 구하면 최솟값이 하나가 아닌 울퉁불퉁한 개형을 가지고 있다. 따라서 다른 방식으로 Cost함수를 계산해야한다.

따라서 위와 같은 방식으로 y값에 따라 적용하는 수식을 달리하여 Cost를 계산한다. 이를 통해 y = 1 일때, H(x)값이 1에 가까울 수 록 Cost 값은 0이되고 H(x)값이 0에 가까울 수 록 Cost값이 증가한다. y = 0 일때는 이와 반대이다.

이와 같이 Cost 함수가 Non-convex하여 극소값이 여럿 존재할 경우 Cost함수를 Convex형태로 전환하여 계산할 수 있고, 이러한 방법을 Cross Entropy Loss라 한다.


