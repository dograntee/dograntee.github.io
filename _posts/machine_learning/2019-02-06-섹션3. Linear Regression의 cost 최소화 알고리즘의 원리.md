---
layout: post
title: "섹션3. Linear Regression"
date: 2019-02-06 19:32:00
image: '/assets/img/ML/intro.jpeg'
description: ML study
category: 'Machine Learning'
tags:
- Machine Learning
twitter_text: Lorem ipsum dolor sit amet, consectetur adipisicing elit.
introduction: A summary of what i studied
---

## 섹션3. Linear Regression

### 00xHypothesis and Cost
 ![problem](/assets/img/ML/section3/figure1.PNG "Hypothesis and Cost")
<center><font size="0.5em">(Fig 1. Hypothesis and Cost)</font></center><br>

가설식에서 b를 0으로 두어, cost 식을 단순화한다.

 X | Y
 ---- | ----
 1 | 1
 2 | 2
 3 | 3

위 데이터를 이용하여 cost(W)를 계산하면 다음과 같다.
 - W = 1, cost(W) = 0<br>
    &nbsp;1/3((1\*1-1)^2 + (1\*2-2)^2 + (1\*3-3)^2)
 - W = 0, cost(W) = 4.67<br>
    &nbsp;1/3((0\*1-1)^2 + (0\*2-2)^2 + (0\*3-3)^2)
 - W = 2, cost(W) = 4.67<br>
    &nbsp;1/3((2\*1-1)^2 + (2\*2-2)^2 + (2\*3-3)^2)

### 01xGradient descent algorithm

 ![problem](/assets/img/ML/section3/figure2.PNG "Hypothesis and Cost")
<center><font size="0.5em">(Fig 2. Hypothesis and Cost)</font></center><br>

cost의 그래프는 위와 같은 모습을 띈다. cost의 최소값을 구하기 위해서는 사용되는 알고리즘 이름은 어렵지만 사실 극소값을 구하는 방식과 동일하다.

 ![problem](/assets/img/ML/section3/figure3.PNG "Formal definition")
<center><font size="0.5em">(Fig 3. Formal definition)</font></center><br>

최솟값인지 검사할 다음 W 값은 현재의 기울기를 일정 비율 a를 곱한 뒤 현재 W값에서 뺀 값이다. 즉 a만큼의 비율로 현재 W에서 극솟값으로 다가가는 형태이다.<br>

### 02xConvex function

 ![problem](/assets/img/ML/section3/figure4.PNG "Convex function")
<center><font size="0.5em">(Fig 4. Convex function)</font></center><br>

3차원 이상의 그래프의 경우 각 지점에서 위와 같은 방식으로 다가가는 극솟값이 달라질 수 있다. 한 지점으로 모이기 위해서는 그래프가 3차원일때 밥그릇 모양과 같이 모든 방향에서 한 극솟값으로 다가가야 하는데 이러한 함수들을 Convex function이라고 한다. 따라서 cost function의 최솟값을 구하기 전에 cost function의 모양이 Fig.4 우측과 같이 convex한지 확인한 뒤 진행해야 한다.